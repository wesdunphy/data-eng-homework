{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('yellow/2024/10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count trips that only started on the 15th of october\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import to_date, lit\n",
    "\n",
    "# Rename cols, convert to date time, filter to pickup_date of October 15\n",
    "df = df\\\n",
    "        .withColumnRenamed('tpep_pickup_datetime', 'pickup_datetime') \\\n",
    "        .withColumnRenamed('tpep_dropoff_datetime', 'dropoff_datetime') \\\n",
    "        \n",
    "df = df.withColumn('pickup_date', F.to_date(df.pickup_datetime))\n",
    "\n",
    "df\\\n",
    "    .filter(df.pickup_date == lit('2024-10-15')) \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, expr\n",
    "\n",
    "df_with_duration = df.withColumn(\n",
    "    \"trip_duration_hours\",\n",
    "    (unix_timestamp(\"dropoff_datetime\") - unix_timestamp(\"pickup_datetime\")) / 3600\n",
    ")\n",
    "\n",
    "longest_trip = df_with_duration.agg({\"trip_duration_hours\": \"max\"}).collect()[0][0]\n",
    "\n",
    "print(f\"The longest trip duration is {longest_trip} hours\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lookup = spark.read \\\n",
    "    .options(header=True, inferSchema=True) \\\n",
    "    .csv('taxi_zone_lookup.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lookup.registerTempTable('lookup_table')\n",
    "df.registerTempTable('yellow_taxi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        Zone,\n",
    "        COUNT(*)\n",
    "    FROM yellow_taxi AS yt\n",
    "    INNER JOIN lookup_table AS lt\n",
    "        ON yt.PULocationID = lt.LocationID\n",
    "    GROUP BY Zone\n",
    "    ORDER BY 2 ASC;\n",
    "    \"\"\"\n",
    ").show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
